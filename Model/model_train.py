# -*- coding: utf-8 -*-
"""Model v3

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/191gZxthnPrgnLZX4mWhP-9cjF1bqvxlO

# Unfake (BERT+CNN)
"""

# %% ----- Environment Setup -----
!pip install torch transformers optuna scikit-learn matplotlib seaborn imbalanced-learn nltk wordcloud > /dev/null

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

# %% ----- Configuration & Constants -----
import os
import random
import numpy as np
import pandas as pd
import re  # For regex operations
import torch
from torch import nn, optim
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup
import optuna
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score, roc_curve, auc
from sklearn.model_selection import train_test_split
from imblearn.under_sampling import RandomUnderSampler
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from wordcloud import WordCloud

class Config:
    """Central configuration management"""
    # System
    SEED = 42
    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    NUM_WORKERS = 4  # Number of workers for DataLoader

    # Data
    DATA_PATH = '/content/drive/My Drive/WELFake_Dataset.csv'
    MAX_LEN = 512
    TEXT_COLUMNS = ['title', 'text']
    LABEL_COLUMN = 'label'
    TEST_SIZE = 0.2
    VAL_SIZE = 0.1

    # Model
    BASE_MODEL = 'bert-base-uncased'
    DROPOUT_RANGE = (0.3, 0.5)
    MODEL_SAVE_PATH = '/content/drive/My Drive/Unfake.pth'

    # Training
    OPTUNA_TRIALS = 10
    EPOCHS = 10
    BATCH_SIZES = [16, 32, 64]
    LEARNING_RATE_RANGE = (2e-5, 5e-5)
    WEIGHT_DECAY_RANGE = (1e-6, 1e-2)
    EARLY_STOP_PATIENCE = 2

    @classmethod
    def setup(cls):
        """Initialize environment"""
        random.seed(cls.SEED)
        np.random.seed(cls.SEED)
        torch.manual_seed(cls.SEED)
        torch.cuda.manual_seed_all(cls.SEED)
        torch.backends.cudnn.deterministic = True

    print("Environment setup complete with SEED:", SEED)

Config.setup()

# %% ----- Exploratory Data Analysis -----
def perform_eda():
    """Perform exploratory data analysis with visualizations."""
    df = pd.read_csv(Config.DATA_PATH, usecols=[0, 1, 2])

    # Required cleaning before analysis
    df = df.dropna(subset=[Config.LABEL_COLUMN])
    df = df[df[Config.LABEL_COLUMN].astype(str).str.strip().isin(["Real", "Fake"])]

    # Basic dataset overview
    print("=== Dataset Overview ===")
    print(f"Shape: {df.shape}")
    print("\nDataset Info:")
    print(df.info())
    print("\nMissing Values:")
    print(df.isnull().sum())
    print("\nDuplicated Values:")
    print(df.duplicated().sum())
    print("\n")

    # Visualizations
    plt.figure(figsize=(18, 12))

    # 1. Class Distribution
    plt.subplot(2, 2, 1)
    sns.countplot(x=Config.LABEL_COLUMN, data=df)
    plt.title("Class Distribution")

    # 2. Text Length Distribution (using the 'text' column)
    df['text_length'] = df['text'].apply(lambda x: len(str(x).split()))
    plt.subplot(2, 2, 2)
    sns.histplot(df['text_length'], bins=50, kde=True)
    plt.title("Text Length Distribution")
    plt.xlim(0, 2000)

    # 3. Word Cloud for Real News
    real_text = " ".join(df[df[Config.LABEL_COLUMN] == 'Real']['text'].astype(str))
    plt.subplot(2, 2, 3)
    wordcloud_real = WordCloud(width=800, height=400, background_color='white').generate(real_text)
    plt.imshow(wordcloud_real, interpolation='bilinear')
    plt.title("Real News Word Cloud")
    plt.axis("off")

    # 4. Word Cloud for Fake News
    fake_text = " ".join(df[df[Config.LABEL_COLUMN] == 'Fake']['text'].astype(str))
    plt.subplot(2, 2, 4)
    wordcloud_fake = WordCloud(width=800, height=400, background_color='white').generate(fake_text)
    plt.imshow(wordcloud_fake, interpolation='bilinear')
    plt.title("Fake News Word Cloud")
    plt.axis("off")

    plt.tight_layout()
    plt.show()

# Perform EDA
perform_eda()

# %% ----- Pre-tokenized Dataset Class -----
class PreTokenizedNewsDataset(Dataset):
    """
    A dataset that uses pre-tokenized texts.
    The tokenized data is expected to be in a dictionary with keys:
    'input_ids', 'attention_mask', and 'labels'.
    """
    def __init__(self, encodings):
        self.encodings = encodings

    def __len__(self):
        return len(self.encodings['input_ids'])

    def __getitem__(self, idx):
        return {
            'input_ids': torch.tensor(self.encodings['input_ids'][idx], dtype=torch.long),
            'attention_mask': torch.tensor(self.encodings['attention_mask'][idx], dtype=torch.long),
            'label': torch.tensor(self.encodings['labels'][idx], dtype=torch.long)
        }

# %% ----- Data Cleaning, Preprocessing & DataLoader Setup -----
class DataManager:
    def __init__(self):
        self.df = self._load_data()
        self._preprocess()
        print(f"Number of entries after cleaning and preprocessing: {len(self.df)}")

    def _load_data(self):
        df = pd.read_csv(Config.DATA_PATH, usecols=[0, 1, 2])

        # Remove duplicates and rows with missing values in text or label columns.
        df.drop_duplicates(inplace=True)
        df.dropna(subset=Config.TEXT_COLUMNS + [Config.LABEL_COLUMN], inplace=True)
        # Keep only entries where the label is either 'Real' or 'Fake'.
        df = df[df[Config.LABEL_COLUMN].astype(str).isin(['Real', 'Fake'])]
        # Combine text columns into a single 'content' field.
        df['content'] = df[Config.TEXT_COLUMNS].astype(str).agg(' '.join, axis=1)

        # Cleaning function to remove extra whitespace and filter out gibberish.
        def clean_text(text):
            # Remove extra spaces.
            text = re.sub(r'\s+', ' ', text).strip()
            # Simple heuristic: if less than 30% of the characters are alphabetic, consider it gibberish.
            if text:
                alpha_ratio = sum(c.isalpha() for c in text) / len(text)
                if alpha_ratio < 0.3:
                    return ''
            return text

        df['content'] = df['content'].apply(clean_text)
        df = df[df['content'] != '']

        # Map labels: if label is "Real", map to 1; if "Fake", map to 0.
        df[Config.LABEL_COLUMN] = df[Config.LABEL_COLUMN].apply(lambda x: 1 if str(x) == 'Real' else 0)
        return df

    def _preprocess(self):
        # Balance classes using random undersampling
        sampler = RandomUnderSampler(random_state=Config.SEED)
        X_resampled, y_resampled = sampler.fit_resample(self.df[['content']], self.df[Config.LABEL_COLUMN])
        self.df = X_resampled.copy()
        self.df[Config.LABEL_COLUMN] = y_resampled

    def get_datasets(self, tokenizer):
        # Split into training, validation, and test sets
        train_df, test_df = train_test_split(
            self.df,
            test_size=Config.TEST_SIZE,
            stratify=self.df[Config.LABEL_COLUMN],
            random_state=Config.SEED
        )
        train_df, val_df = train_test_split(
            train_df,
            test_size=Config.VAL_SIZE,
            stratify=train_df[Config.LABEL_COLUMN],
            random_state=Config.SEED
        )

        # ---- Data Leakage Check ----
        # Using the 'content' column to verify that there is no overlap
        overlap_train_val = set(train_df['content']).intersection(set(val_df['content']))
        overlap_train_test = set(train_df['content']).intersection(set(test_df['content']))
        overlap_val_test = set(val_df['content']).intersection(set(test_df['content']))

        # You can choose to either assert or handle overlaps gracefully.
        if len(overlap_train_val) > 0:
            raise AssertionError(f"Data leakage: {len(overlap_train_val)} overlapping samples between train and validation sets.")

        if len(overlap_train_test) > 0:
            print(f"Warning: Found {len(overlap_train_test)} overlapping sample(s) between train and test sets. Removing them from the test set.")
            test_df = test_df[~test_df['content'].isin(overlap_train_test)]

        if len(overlap_val_test) > 0:
            raise AssertionError(f"Data leakage: {len(overlap_val_test)} overlapping samples between validation and test sets.")

        print("No data leakage detected between train, validation, and test splits (after removal of test duplicates).")

        print("Pre-tokenizing training, validation, and test data...")
        # Pre-tokenize using batch_encode_plus
        train_encodings = tokenizer.batch_encode_plus(
            train_df['content'].tolist(),
            max_length=Config.MAX_LEN,
            padding='max_length',
            truncation=True,
            return_attention_mask=True
        )
        val_encodings = tokenizer.batch_encode_plus(
            val_df['content'].tolist(),
            max_length=Config.MAX_LEN,
            padding='max_length',
            truncation=True,
            return_attention_mask=True
        )
        test_encodings = tokenizer.batch_encode_plus(
            test_df['content'].tolist(),
            max_length=Config.MAX_LEN,
            padding='max_length',
            truncation=True,
            return_attention_mask=True
        )

        # Add labels
        train_encodings['labels'] = train_df[Config.LABEL_COLUMN].tolist()
        val_encodings['labels'] = val_df[Config.LABEL_COLUMN].tolist()
        test_encodings['labels'] = test_df[Config.LABEL_COLUMN].tolist()

        print("Creating pre-tokenized datasets...")
        return {
            'train': PreTokenizedNewsDataset(train_encodings),
            'val': PreTokenizedNewsDataset(val_encodings),
            'test': PreTokenizedNewsDataset(test_encodings)
        }

# %% ----- Model Building (BERT + CNN) -----
class FakeNewsClassifier(nn.Module):
    def __init__(self, dropout_rate=0.3):
        super().__init__()
        self.bert = BertModel.from_pretrained(Config.BASE_MODEL)
        self.dropout = nn.Dropout(dropout_rate)

        # Parallel CNN blocks with different kernel sizes
        self.conv3 = nn.Conv1d(self.bert.config.hidden_size, 128, kernel_size=3, padding=1)
        self.conv4 = nn.Conv1d(self.bert.config.hidden_size, 128, kernel_size=4, padding=1)
        self.conv5 = nn.Conv1d(self.bert.config.hidden_size, 128, kernel_size=5, padding=1)

        self.pool = nn.AdaptiveMaxPool1d(1)
        self.activation = nn.ReLU()

        # Combined layers
        self.classifier = nn.Sequential(
            nn.Linear(384, 128),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(128, 2)
        )

    def forward(self, input_ids, attention_mask):
        # BERT processing
        bert_output = self.bert(input_ids, attention_mask)
        sequence_output = bert_output.last_hidden_state.permute(0, 2, 1)  # [batch, hidden_size, seq_len]

        # Parallel CNN processing
        conv3_out = self.pool(self.activation(self.conv3(sequence_output))).squeeze(-1)
        conv4_out = self.pool(self.activation(self.conv4(sequence_output))).squeeze(-1)
        conv5_out = self.pool(self.activation(self.conv5(sequence_output))).squeeze(-1)

        # Concatenate CNN outputs
        combined = torch.cat([conv3_out, conv4_out, conv5_out], dim=1)

        # Classification
        logits = self.classifier(self.dropout(combined))
        return logits

# %% ----- Training Utilities -----
class EarlyStopping:
    def __init__(self, patience=Config.EARLY_STOP_PATIENCE, delta=0):
        self.patience = patience
        self.delta = delta
        self.counter = 0
        self.best_score = None
        self.early_stop = False

    def __call__(self, val_loss):
        score = -val_loss
        if self.best_score is None:
            self.best_score = score
        elif score < self.best_score + self.delta:
            self.counter += 1
            print(f"EarlyStopping counter: {self.counter} out of {self.patience}")
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_score = score
            self.counter = 0

class Trainer:
    def __init__(self, model):
        self.model = model.to(Config.DEVICE)
        self.early_stopping = EarlyStopping(patience=Config.EARLY_STOP_PATIENCE)
        self.best_val_loss = float('inf')
        self.best_model = None

    def train_epoch(self, loader, optimizer, scheduler):
        self.model.train()
        total_loss = 0
        all_preds = []
        all_labels = []
        for batch_idx, batch in enumerate(loader):
            optimizer.zero_grad()
            inputs = {k: v.to(Config.DEVICE) for k, v in batch.items() if k != 'label'}
            labels = batch['label'].to(Config.DEVICE)

            logits = self.model(**inputs)
            loss = nn.CrossEntropyLoss()(logits, labels)
            loss.backward()
            optimizer.step()
            scheduler.step()
            total_loss += loss.item()

            # Accumulate predictions for training accuracy calculation
            preds = torch.argmax(logits, dim=1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

            if (batch_idx + 1) % 10 == 0:
                print(f"Batch {batch_idx+1}/{len(loader)}: Loss = {loss.item():.4f}")
        avg_loss = total_loss / len(loader)
        train_acc = accuracy_score(all_labels, all_preds)
        return avg_loss, train_acc

    def evaluate(self, loader):
        self.model.eval()
        all_probs, all_labels = [], []
        total_loss = 0
        with torch.no_grad():
            for batch in loader:
                inputs = {k: v.to(Config.DEVICE) for k, v in batch.items() if k != 'label'}
                labels = batch['label'].to(Config.DEVICE)

                logits = self.model(**inputs)
                loss = nn.CrossEntropyLoss()(logits, labels)
                probs = torch.softmax(logits, dim=1).cpu()

                total_loss += loss.item()
                all_probs.extend(probs.numpy())
                all_labels.extend(labels.cpu().numpy())
        return total_loss / len(loader), all_probs, all_labels

    def optimize_hyperparameters(self, train_data, val_data):
        def objective(trial):
            params = {
                'batch_size': trial.suggest_categorical('batch_size', Config.BATCH_SIZES),
                'lr': trial.suggest_float('lr', *Config.LEARNING_RATE_RANGE, log=True),
                'dropout': trial.suggest_float('dropout', *Config.DROPOUT_RANGE),
                'weight_decay': trial.suggest_float('weight_decay', *Config.WEIGHT_DECAY_RANGE, log=True)
            }
            print(f"Trial {trial.number}: Testing params {params}")

            model = FakeNewsClassifier(dropout_rate=params['dropout']).to(Config.DEVICE)
            # Include weight decay here
            optimizer = AdamW(model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'])
            train_loader = DataLoader(train_data, batch_size=params['batch_size'], shuffle=True, num_workers=Config.NUM_WORKERS)
            val_loader = DataLoader(val_data, batch_size=params['batch_size'], num_workers=Config.NUM_WORKERS)
            scheduler = get_linear_schedule_with_warmup(optimizer, 0, len(train_loader)*5)

            # Create a trainer for the current trial's model
            trial_trainer = Trainer(model)

            for epoch in range(5):  # 3 epochs per trial
                print(f"Trial {trial.number}, Epoch {epoch+1}: Training...")
                trial_trainer.train_epoch(train_loader, optimizer, scheduler)
                val_loss, val_probs, val_labels = trial_trainer.evaluate(val_loader)
                intermediate_value = roc_auc_score(val_labels, [p[1] for p in val_probs])
                print(f"Trial {trial.number}, Epoch {epoch+1}: Val Loss = {val_loss:.4f}, AUC = {intermediate_value:.4f}")
                trial.report(intermediate_value, epoch)

                if trial.should_prune():
                    print(f"Trial {trial.number} pruned at epoch {epoch+1}")
                    raise optuna.TrialPruned()

            final_auc = roc_auc_score(val_labels, [p[1] for p in val_probs])
            print(f"Trial {trial.number} finished with final AUC = {final_auc:.4f}")
            return final_auc

        study = optuna.create_study(direction='maximize')
        print("Starting hyperparameter optimization...")
        study.optimize(objective, n_trials=Config.OPTUNA_TRIALS)
        print("Hyperparameter optimization completed.")
        print(f"Best params: {study.best_params}")
        return study.best_params

# %% ----- Evaluation Metrics -----
def plot_confusion_matrix(y_true, y_pred):
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(6, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=['Fake', 'Real'],
                yticklabels=['Fake', 'Real'])
    plt.ylabel('Actual')
    plt.xlabel('Predicted')
    plt.title('Confusion Matrix')
    plt.show()

def plot_roc(y_true, probs):
    fpr, tpr, _ = roc_curve(y_true, [p[1] for p in probs])
    roc_auc = auc(fpr, tpr)
    plt.figure()
    plt.plot(fpr, tpr, color='darkorange', lw=2,
             label=f'ROC (AUC = {roc_auc:.2f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve')
    plt.legend(loc="lower right")
    plt.show()

# %% ----- Main Execution -----
def main():
    print("Starting main execution...")

    # 1. (Optional) Perform EDA
    # perform_eda()

    # 2. Initialize tokenizer and prepare datasets
    tokenizer = BertTokenizer.from_pretrained(Config.BASE_MODEL)
    data_manager = DataManager()
    datasets = data_manager.get_datasets(tokenizer)

    # 3. Hyperparameter Optimization
    base_model = FakeNewsClassifier().to(Config.DEVICE)
    trainer = Trainer(base_model)
    best_params = trainer.optimize_hyperparameters(datasets['train'], datasets['val'])
    print(f"Best Hyperparameters: {best_params}")

    # 4. Final Training Setup using Best Hyperparameters
    model = FakeNewsClassifier(dropout_rate=best_params['dropout']).to(Config.DEVICE)
    optimizer = AdamW(model.parameters(), lr=best_params['lr'], weight_decay=best_params['weight_decay'])
    train_loader = DataLoader(datasets['train'], batch_size=best_params['batch_size'], shuffle=True, num_workers=Config.NUM_WORKERS)
    val_loader = DataLoader(datasets['val'], batch_size=best_params['batch_size'], num_workers=Config.NUM_WORKERS)

    total_steps = len(train_loader) * Config.EPOCHS
    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=0,
        num_training_steps=total_steps
    )

    trainer = Trainer(model)
    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}

    # Set initial best validation loss to a high value so the first epoch always saves a checkpoint
    trainer.best_val_loss = float('inf')

    for epoch in range(Config.EPOCHS):
        print(f"Epoch {epoch+1}/{Config.EPOCHS} - Training started")
        train_loss, train_acc = trainer.train_epoch(train_loader, optimizer, scheduler)
        val_loss, val_probs, val_labels = trainer.evaluate(val_loader)
        val_preds = np.argmax(val_probs, axis=1)
        val_acc = accuracy_score(val_labels, val_preds)

        history['train_loss'].append(train_loss)
        history['train_acc'].append(train_acc)
        history['val_loss'].append(val_loss)
        history['val_acc'].append(val_acc)

        print(f"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Train Acc = {train_acc:.4f}, Val Loss = {val_loss:.4f}, Val Acc = {val_acc:.4f}")

        # Check for early stopping
        trainer.early_stopping(val_loss)
        if trainer.early_stopping.early_stop:
            print(f"Early stopping triggered at epoch {epoch+1}")
            break

        # Save a checkpoint if the current validation loss is better than the best seen so far
        if val_loss < trainer.best_val_loss:
            trainer.best_val_loss = val_loss
            trainer.best_model = model.state_dict()
            # Save checkpoint to disk (to your Google Drive)
            torch.save(model.state_dict(), Config.MODEL_SAVE_PATH)
            print(f"Checkpoint saved to {Config.MODEL_SAVE_PATH} at epoch {epoch+1}")

    # Plot training vs. validation accuracy over epochs
    plt.figure(figsize=(10, 5))
    epochs_range = range(1, len(history['train_acc']) + 1)
    plt.plot(epochs_range, history['train_acc'], label='Train Accuracy')
    plt.plot(epochs_range, history['val_acc'], label='Validation Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.title('Train vs. Validation Accuracy')
    plt.legend()
    plt.show()
    print("\n")

    # Plot training vs. validation loss over epochs
    plt.figure(figsize=(10, 5))
    plt.plot(epochs_range, history['train_loss'], label='Train Loss')
    plt.plot(epochs_range, history['val_loss'], label='Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Train vs. Validation Loss')
    plt.legend()
    plt.show()
    print("\n")

    # Load best model weights before final evaluation
    model.load_state_dict(trainer.best_model)
    print("Final training completed. Best model loaded.")

    # 5. Final Evaluation on Test Set
    test_loader = DataLoader(datasets['test'], batch_size=best_params['batch_size'], num_workers=Config.NUM_WORKERS)
    test_loss, test_probs, test_labels = trainer.evaluate(test_loader)
    test_preds = np.argmax(test_probs, axis=1)

    print("Test evaluation results:")
    print(f"Test Loss: {test_loss:.4f}")
    print(f"Test Accuracy: {accuracy_score(test_labels, test_preds):.4f}")
    print(f"Test AUC: {roc_auc_score(test_labels, [p[1] for p in test_probs]):.4f}")
    print("\nClassification Report:")
    print(classification_report(test_labels, test_preds, target_names=['Fake', 'Real']))

    plot_confusion_matrix(test_labels, test_preds)
    print("\n")
    plot_roc(test_labels, test_probs)
    print("\n")

    # 6. Save the Best Model (final checkpoint)
    torch.save(model.state_dict(), Config.MODEL_SAVE_PATH)
    print(f"Final model saved to {Config.MODEL_SAVE_PATH}")


if __name__ == '__main__':
    main()

"""# Testing"""

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

import torch
import pandas as pd
import numpy as np
import re
from torch import nn
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer, BertModel

# ------------------ Configuration ------------------
class Config:
    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    BASE_MODEL = 'bert-base-uncased'
    MAX_LEN = 512
    MODEL_SAVE_PATH = '/content/drive/My Drive/Colab Notebooks/Capstone Model/Unfake_v3.pth'
    TEST_DATA_PATH = '/content/drive/My Drive/Colab Notebooks/Capstone Dataset/raw_tweets_extracted.csv'
    BATCH_SIZE = 32
    NUM_WORKERS = 4
    TEMPERATURE = 4.0

# ------------------ Model Definition ------------------
class FakeNewsClassifier(nn.Module):
    def __init__(self, dropout_rate=0.3):
        super().__init__()
        self.bert = BertModel.from_pretrained(Config.BASE_MODEL)
        self.dropout = nn.Dropout(dropout_rate)
        # Three parallel CNN blocks with kernel sizes 3, 4, and 5
        self.conv3 = nn.Conv1d(self.bert.config.hidden_size, 128, kernel_size=3, padding=1)
        self.conv4 = nn.Conv1d(self.bert.config.hidden_size, 128, kernel_size=4, padding=1)
        self.conv5 = nn.Conv1d(self.bert.config.hidden_size, 128, kernel_size=5, padding=1)
        self.pool = nn.AdaptiveMaxPool1d(1)
        self.activation = nn.ReLU()
        self.classifier = nn.Sequential(
            nn.Linear(384, 128),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(128, 2)
        )

    def forward(self, input_ids, attention_mask):
        bert_output = self.bert(input_ids, attention_mask)
        # Permute output to shape (batch, hidden_size, seq_len) for CNN layers
        sequence_output = bert_output.last_hidden_state.permute(0, 2, 1)
        conv3_out = self.pool(self.activation(self.conv3(sequence_output))).squeeze(-1)
        conv4_out = self.pool(self.activation(self.conv4(sequence_output))).squeeze(-1)
        conv5_out = self.pool(self.activation(self.conv5(sequence_output))).squeeze(-1)
        combined = torch.cat([conv3_out, conv4_out, conv5_out], dim=1)
        return self.classifier(self.dropout(combined))

# ------------------ Dataset for Inference ------------------
class TestDataset(Dataset):
    def __init__(self, texts, tokenizer, max_len=Config.MAX_LEN):
        self.encodings = tokenizer.batch_encode_plus(
            texts,
            max_length=max_len,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt'
        )

    def __len__(self):
        return self.encodings['input_ids'].shape[0]

    def __getitem__(self, idx):
        return {key: val[idx] for key, val in self.encodings.items()}

# ------------------ Utility Function ------------------
def clean_text(text):
    """
    Remove extra whitespace and filter out texts with less than 30% alphabetic characters.
    """
    text = re.sub(r'\s+', ' ', str(text)).strip()
    if text and (sum(c.isalpha() for c in text) / len(text)) < 0.3:
        return ''
    return text

# ------------------ Main Inference Script ------------------
def main():
    # Load the data CSV file
    df = pd.read_csv(Config.TEST_DATA_PATH)

    # Clean the 'tweet_text' field
    df['tweet_text'] = df['tweet_text'].astype(str).apply(clean_text)
    df = df[df['tweet_text'] != '']

    texts = df['tweet_text'].tolist()

    # Initialize the tokenizer and create the dataset.
    tokenizer = BertTokenizer.from_pretrained(Config.BASE_MODEL)
    test_dataset = TestDataset(texts, tokenizer, Config.MAX_LEN)
    test_loader = DataLoader(test_dataset, batch_size=Config.BATCH_SIZE, num_workers=Config.NUM_WORKERS)

    # Load the saved model.
    model = FakeNewsClassifier()
    model.load_state_dict(torch.load(Config.MODEL_SAVE_PATH, map_location=Config.DEVICE))
    model.to(Config.DEVICE)
    model.eval()

    softmax = nn.Softmax(dim=1)
    predictions, probabilities_list = [], []

    # Run inference with temperature scaling.
    with torch.no_grad():
        for batch in test_loader:
            input_ids = batch['input_ids'].to(Config.DEVICE)
            attention_mask = batch['attention_mask'].to(Config.DEVICE)
            logits = model(input_ids, attention_mask)
            scaled_logits = logits / Config.TEMPERATURE
            probs = softmax(scaled_logits)
            probs_np = probs.cpu().numpy()
            preds = np.argmax(probs_np, axis=1)
            predictions.extend(preds.tolist())
            probabilities_list.extend(probs_np.tolist())

    df["Predicted_Score"] = predictions
    df["Probability_Real"] = [prob[1] for prob in probabilities_list]
    df["Probability_Fake"] = [prob[0] for prob in probabilities_list]

    # Calculate entropy using the formula:
    # Entropy = -(P_Real * log2(P_Real) + P_Fake * log2(P_Fake))
    # Using np.where to avoid log2(0) by substituting 1 (since log2(1) = 0).
    df["Entropy"] = -(
        df["Probability_Real"] * np.log2(np.where(df["PSrobability_Real"] > 0, df["Probability_Real"], 1)) +
        df["Probability_Fake"] * np.log2(np.where(df["Probability_Fake"] > 0, df["Probability_Fake"], 1))
    )
S
    output_file = "test_predictions_v3.csv"
    df.to_csv(output_file, index=False)
    print(f"Predictions saved to {output_file}")

if __name__ == "__main__":
    main()